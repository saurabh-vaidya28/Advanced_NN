### 1. What is the role of an optimizer in a neural network?
Answer: An optimizer’s role in a neural network is to update the network's weights during training by minimizing the loss function. The optimizer adjusts the weights based on the gradients (calculated via backpropagation), ensuring the network moves toward better performance with each iteration. By doing so, the optimizer helps the network learn from data and generalize well to unseen data.

The optimizer’s role is to find the best combination of weights and biases that leads to the most accurate predictions.

### Gradient Descent
Gradient Descent is a popular optimization method for training machine learning models. It works by iteratively adjusting the model parameters in the direction that minimizes the loss function.

Key Steps in Gradient Descent

1. Initialize parameters: Randomly initialize the model parameters.
2. Compute the gradient: Calculate the gradient (derivative) of the loss function with respect to the parameters.
3. Update parameters: Adjust the parameters by moving in the opposite direction of the gradient, scaled by the learning rate.

### Variants of Gradient Descent:

1. Stochastic Gradient Descent (SGD)
Stochastic Gradient Descent (SGD) updates the model parameters after each training example, making it more efficient for large datasets compared to traditional Gradient Descent, which uses the entire dataset for each update.

Steps:
1. Select a training example.
2. Compute the gradient of the loss function.
3. Update the model parameters.

- Advantages: Requires less memory and may find new minima.
- Disadvantages: Noisier, requiring more iterations to converge.
