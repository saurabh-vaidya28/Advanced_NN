1. What is the role of an optimizer in a neural network?
Answer: An optimizerâ€™s role in a neural network is to update the network's weights during training by minimizing the loss function. The optimizer adjusts the weights based on the gradients (calculated via backpropagation), ensuring the network moves toward better performance with each iteration. By doing so, the optimizer helps the network learn from data and generalize well to unseen data.

2. What is the difference between Stochastic Gradient Descent (SGD) and Mini-Batch Gradient Descent?
Answer:

Stochastic Gradient Descent (SGD): In this approach, the weights are updated after each training sample. While it can result in noisy updates, it often helps the model escape local minima and explore the solution space more effectively.

Mini-Batch Gradient Descent: Instead of updating the weights after each individual sample, mini-batch gradient descent uses a small subset of the training data (mini-batch) to calculate gradients. This leads to more stable updates compared to SGD, and it also benefits from parallel computation on GPUs, making it more efficient.
