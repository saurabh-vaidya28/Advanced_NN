## 1. What is forward propagation in a neural network?
Answer: Forward propagation is the process through which input data passes through the network to generate an output. It involves computing the weighted sum of inputs at each layer, adding a bias, applying an activation function, and passing the result to the next layer. The final output layer produces the network's prediction.<br>
The steps are:<br>
- Compute the weighted sum of inputs.
- Add bias.
- Apply an activation function (like ReLU, Sigmoid, etc.).
- Repeat for all layers.
- Output the final prediction (in classification, usually through a sigmoid or softmax function).
<br>

## 2. Explain the formula for forward propagation in a neural network.<br>
Answer: In a neural network, forward propagation computes the activations layer by layer. The general formula for a single layer is:<br>

ğ‘<sup>[ğ‘™]</sup> = ğ‘Š<sup>[ğ‘™]</sup> ğ´<sup>[ğ‘™-1]</sup> + ğ‘<sup>[ğ‘™]</sup>

Where:
- Z<sup>[ğ‘™]</sup> is the weighted sum for the l-th layer.
- W<sup>[ğ‘™]</sup> is the weight matrix for the l-th layer.
- A<sup>[ğ‘™-1]</sup> is the activation output from the previous layer.
- b<sup>[ğ‘™]</sup> is the bias vector for the l-th layer.

Then, the activation function (e.g., ReLU, Sigmoid, or Tanh) is applied to Z<sup>[ğ‘™]</sup>:<br>
ğ´<sup>[ğ‘™]</sup> = activation(ğ‘<sup>[ğ‘™]</sup>)<br>
This process repeats for all layers until the final output layer is reached.
<br>

## 3. What role does the activation function play in forward propagation?<br>
Answer: The activation function introduces non-linearity into the neural network. Without an activation function, the network would simply perform linear transformations, no matter how many layers there are. This would limit the networkâ€™s ability to model complex patterns.

For example:<br>
- ReLU (Rectified Linear Unit) is commonly used in hidden layers and helps prevent vanishing gradients.
- Sigmoid or softmax is used in the output layer for classification tasks, converting raw output values into probabilities.

The activation function ensures that the network can learn and approximate complex functions by allowing it to model non-linear relationships.
<br>

## 4. What is the difference between forward propagation and backpropagation?<br>
Answer:
- Forward Propagation is the process of passing input through the network to generate predictions. It involves computing the weighted sum, applying the activation function, and propagating the output layerâ€™s prediction.

- Backpropagation is the process of updating the weights and biases of the network by calculating gradients. After forward propagation, backpropagation uses the error (difference between predicted and actual outputs) to adjust the weights and minimize the error using optimization techniques (like gradient descent).

In short, forward propagation generates the predictions, while backpropagation helps improve the model by learning from errors.













