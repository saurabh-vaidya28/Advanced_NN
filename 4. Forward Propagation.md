## 1. What is forward propagation in a neural network?
Answer: Forward propagation is the process through which input data passes through the network to generate an output. It involves computing the weighted sum of inputs at each layer, adding a bias, applying an activation function, and passing the result to the next layer. The final output layer produces the network's prediction.<br>
The steps are:<br>
- Compute the weighted sum of inputs.
- Add bias.
- Apply an activation function (like ReLU, Sigmoid, etc.).
- Repeat for all layers.
- Output the final prediction (in classification, usually through a sigmoid or softmax function).
<br>

## 2. Explain the formula for forward propagation in a neural network.<br>
Answer: In a neural network, forward propagation computes the activations layer by layer. The general formula for a single layer is:<br>

𝑍<sup>[𝑙]</sup> = 𝑊<sup>[𝑙]</sup> 𝐴<sup>[𝑙-1]</sup> + 𝑏<sup>[𝑙]</sup>

Where:
- Z<sup>[𝑙]</sup> is the weighted sum for the l-th layer.
- W<sup>[𝑙]</sup> is the weight matrix for the l-th layer.
- A<sup>[𝑙-1]</sup> is the activation output from the previous layer.
- b<sup>[𝑙]</sup> is the bias vector for the l-th layer.

Then, the activation function (e.g., ReLU, Sigmoid, or Tanh) is applied to Z<sup>[𝑙]</sup>:<br>
𝐴<sup>[𝑙]</sup> = activation(𝑍<sup>[𝑙]</sup>)<br>
This process repeats for all layers until the final output layer is reached.
<br>

## 3. What role does the activation function play in forward propagation?<br>
Answer: The activation function introduces non-linearity into the neural network. Without an activation function, the network would simply perform linear transformations, no matter how many layers there are. This would limit the network’s ability to model complex patterns.

For example:<br>
- ReLU (Rectified Linear Unit) is commonly used in hidden layers and helps prevent vanishing gradients.
- Sigmoid is useful for binary classification, mapping outputs to a probability range of [0,1].
- Softmax is used in multi-class classification tasks, converting the network output into a probability distribution.

The activation function ensures that the network can learn and approximate complex functions by allowing it to model non-linear relationships.
<br>

## 4. What is the difference between forward propagation and backpropagation?<br>
Answer:
- Forward Propagation is the process of passing input through the network to generate predictions. It involves computing the weighted sum, applying the activation function, and propagating the output layer’s prediction.

- Backpropagation is the process of updating the weights and biases of the network by calculating gradients. After forward propagation, backpropagation uses the error (difference between predicted and actual outputs) to adjust the weights and minimize the error using optimization techniques (like gradient descent).

In short, forward propagation generates the predictions, while backpropagation helps improve the model by learning from errors.
<br>

## 5. What is a vanishing gradient problem, and how does it affect forward propagation?<br>
Answer: The vanishing gradient problem occurs when gradients become very small during backpropagation, especially in deep neural networks with many layers. As a result, the weights in the earlier layers of the network are updated very slowly, making it hard for the model to learn.

In forward propagation, the outputs can become very small due to activation functions like the sigmoid or tanh, where the derivative becomes small. This makes it harder for the model to adjust weights effectively during backpropagation.

Solution:
- Use activation functions like ReLU that don’t saturate in the positive domain.
- Implement techniques like Batch Normalization and Residual Networks (ResNets) to mitigate vanishing gradients.










