## Gradient Descent:<br>

-> It is a key optimization algorithm used to minimize the loss function in neural network during training.<br>
-> The goal is to adjust the weight and biases in the network to minimize the difference between predicted output and the actual target values (i.e., minimize the loss)<br>

### How Gradient Descent works?
1) Forward Bias:<br>
    - Input data is passed through the neural network.
    - The network produces an output based on its current weight and biases.

2) Compute loss:<br>
    - The difference between the predicted output and the actual value is computred using a loss function (e.g., MSE or Cross-Entropy loss)
    - This indicates how far predictions are from true values.

3) Back Propagation:<br>
    - The error loss is propagated backward through the network to calculate the gradients.
    - The gradients is the partial derivative of loss with respect to each weight in the network. This tells us how much each weight in the network contributed to the error.
  
4) Update Weights:<br>
    - The weights are adjusted in the direction that reduces the error. This is done by subtracting a small portion of the gradients from current weight.

5) Repeat:<br>
    - The above steps are repeated for several iterations until the loss converges to a minimum or stops decreasing significantly.
  
<br>

## Types of Gradient Descent:
1) Batch Gradient Descent:<br>
    a) Compute the gradient of the loss function using the entrie dataset.<br>
    b) It provides a stable but computationally expensive update.<br>

2) Stochastic Gradient Descent:<br>
    a) Computes the gradient and updates the weights using only a single data point.<br>
    b) It is much faster but introduces more variance in the updates, which may result in noisy or fluctuation updates.<br>

3) Mini-Batch Gradient Descent:<br>
    a) A compromise between batch and stochastic gradient descent, where updates are made using small random subsets of the data.<br>
    b) It is faster than batch gradient descent and more stable than stochastic gardient descent.<br>
   
## Advantages of Gradient Descent:
a) Scalable to High - Dimensional Data:<br>
Gradient descent can effectively handle large-scale optimization problems, especially when using Stochastic descent or mini-batch gradient descent.<br>

b) Flexibility & Applicability:<br>
Gradient descent is a general purpose optimization technique that can be applied to a wide range of neural network architectures (e.g., fully connected, convolutional, recurrent) and tasks (e.g., classification, regression, generative models)<br>

c) Efficient for Large Datasets:<br>
For training on massive datasets, gradient descent allows updating the model weights incrementally, which reduces memory requirements and speed ups computation.<br>

d) Easy to implement<br>
e) Works well with Backpropagation<br>
f) Adaptable to different optimizers<br>

+ Dandvantages of Gradient Dexend:

4) Slow Convergence:

Layers networks with many can have slow convergence bater especially when the learning grave is poory charon. Sensitive to Hyperparameters"

Neural network ofeen require careful

tuning of hyperparameter such Learning rate, batch size and mametum. Getting Stuck in Paint Local Mimcima Saddle FO

In the case of non-convex gradient descent can minima Cars function get stuck in local ar saddle pom like Stochastic gradient descent Gradient desc exape these, there's minimum. While techniques can help guarantee of

finding the global df Overfitting: Neural networks

models, and highly experive are high without regularizata techniques (eg despant, 12 regularlatan), adient descent gaudiens might overfit the.. trauming date, especially when training. on small dataset



