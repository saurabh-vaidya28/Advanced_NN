### 1. What is backpropagation in a neural network?
Answer:
Backpropagation (short for "backward propagation of errors") is the algorithm used to minimize the loss function by adjusting the weights of the neural network through gradient descent. It works by computing the gradient of the loss function with respect to each weight and updating the weights to reduce the error between the predicted output and the true target.
Backpropagation consists of two phases:
- Forward Pass: The input is passed through the network to generate predictions.
- Backward Pass: The error is propagated back through the network to update the weights using gradient descent.

<br>
### 2. How does backpropagation work step by step?
Answer:
1. Forward Pass:
    - Compute the activations at each layer using the weights, biases, and activation functions.
    - Calculate the output of the network.

2. Loss Calculation:
    - Compute the loss using a loss function (e.g., Mean Squared Error for regression or Cross-Entropy for classification).

3. Backward Pass:
    - Compute the gradient of the loss with respect to each weight using the chain rule of calculus.
    - The error is propagated backward through the network, from the output layer to the input layer.

4. Gradient Descent:
    - Update the weights and biases by subtracting a fraction of the gradients (scaled by the learning rate).

5. Repeat:
    - This process is repeated for each training sample and through multiple epochs.







