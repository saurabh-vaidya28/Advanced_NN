### 1. Batch Normalization (BN)
What is Batch Normalization? <br>
Batch Normalization is a technique used to normalize the input of each layer, by adjusting and scaling activations so that they have a mean of zero and a standard deviation of one. This is done for each mini-batch of data during training, hence the name.

How Batch Normalization Mitigates the Vanishing Gradient Problem:
- Prevents Activations from Becoming Too Small or Large: During forward propagation, Batch Normalization keeps the activations from growing too large or shrinking too small by ensuring they are centered around zero and have unit variance. This prevents the outputs from becoming saturated, which could lead to vanishing gradients, especially for activation functions like sigmoid or tanh that saturate at extreme values.

- Reduces Internal Covariate Shift: In deep networks, each layer‚Äôs distribution of inputs keeps changing during training (this is called internal covariate shift). Batch Normalization reduces this issue by stabilizing the distribution of layer inputs, making it easier for the model to learn. This in turn leads to more stable gradients during backpropagation, reducing the risk of vanishing gradients.

- Improves Gradient Flow: By normalizing the output of each layer, Batch Normalization ensures that the backpropagated gradients are neither too small (vanishing) nor too large (exploding). This improves the gradient flow throughout the network, making it easier to train very deep networks.

How it works:<br>
- Step 1: For each mini-batch, calculate the mean and variance of the activations.
- Step 2: Normalize the activations to have a mean of 0 and a variance of 1.
- Step 3: Scale and shift the activations using learned parameters ùõæ and ùõΩ, allowing the model to restore the original distribution if needed.

In effect, Batch Normalization "smooths out" the landscape of the loss function, making it easier for the optimizer to find optimal weights and reducing the likelihood of vanishing gradients.

<br>

### 2. Residual Networks (ResNets)
What are Residual Networks (ResNets)? <br>
Residual Networks are a type of deep neural network architecture that includes skip connections (also known as residual connections). These connections allow the network to bypass one or more layers, skipping directly to a later layer, and adding the output of earlier layers to the deeper layers. This means the network doesn‚Äôt have to learn the identity function itself, but can instead learn residuals (the difference from the identity function).

How Residual Networks Mitigate the Vanishing Gradient Problem:
- Skip Connections Preserve Gradient Flow: In a regular deep network, the gradients during backpropagation can diminish as they propagate through many layers (leading to vanishing gradients). With skip connections in ResNets, the gradients can bypass one or more layers, allowing them to flow more easily through the network. This "shortcut" ensures that earlier layers still receive substantial gradients, making it easier for them to learn.

- Easier Gradient Flow through Identity Mapping: In ResNets, instead of learning an entire transformation (i.e., how to map the input to the output), each layer learns the residual ‚Äî the difference between the desired output and the input to that layer. This reduces the complexity of the learning process and makes it easier for the network to propagate gradients because the residual learning can be seen as learning a small change to the identity mapping (which is easier).

- Prevents Degradation in Deep Networks: When a deep network grows deeper, it becomes harder to train due to the vanishing gradients and degradation (where adding more layers results in worse performance). ResNets mitigate this problem by allowing layers to learn identity mappings (if no useful transformation is needed) and effectively make very deep networks trainable.

How it works:<br>
- A ResNet block typically consists of two or more layers, and the output of the block is the sum of the output of these layers and the original input to the block:

          Output=F(x)+x
Where:
- F(x) is the function learned by the network (the transformation applied to the input x).
- x is the original input, passed through a skip connection.

By using skip connections, ResNets make it possible to train networks with hundreds or even thousands of layers without running into the vanishing gradient problem.

### Combined Effect of Batch Normalization and ResNets on Vanishing Gradients<br>
When used together, Batch Normalization and Residual Networks provide a powerful defense against vanishing gradients:

- Batch Normalization keeps activations at healthy scales, reducing the chances of the network's layers becoming saturated (i.e., with very small or very large values), which can lead to vanishing gradients during backpropagation.

- Residual Networks provide an architecture that guarantees the gradient has a path to flow through, even if deeper layers don't contribute much during training. This ensures that gradients can be propagated more easily back to the earlier layers.
