### 1. Batch Normalization (BN)
What is Batch Normalization? <br>
Batch Normalization is a technique used to normalize the input of each layer, by adjusting and scaling activations so that they have a mean of zero and a standard deviation of one. This is done for each mini-batch of data during training, hence the name.

How Batch Normalization Mitigates the Vanishing Gradient Problem:
- Prevents Activations from Becoming Too Small or Large: During forward propagation, Batch Normalization keeps the activations from growing too large or shrinking too small by ensuring they are centered around zero and have unit variance. This prevents the outputs from becoming saturated, which could lead to vanishing gradients, especially for activation functions like sigmoid or tanh that saturate at extreme values.

- Reduces Internal Covariate Shift: In deep networks, each layer‚Äôs distribution of inputs keeps changing during training (this is called internal covariate shift). Batch Normalization reduces this issue by stabilizing the distribution of layer inputs, making it easier for the model to learn. This in turn leads to more stable gradients during backpropagation, reducing the risk of vanishing gradients.

- Improves Gradient Flow: By normalizing the output of each layer, Batch Normalization ensures that the backpropagated gradients are neither too small (vanishing) nor too large (exploding). This improves the gradient flow throughout the network, making it easier to train very deep networks.

How it works:
  Step 1: For each mini-batch, calculate the mean and variance of the activations.
  Step 2: Normalize the activations to have a mean of 0 and a variance of 1.
  Step 3: Scale and shift the activations using learned parameters ùõæ and ùõΩ, allowing the model to restore the original distribution if needed.

In effect, Batch Normalization "smooths out" the landscape of the loss function, making it easier for the optimizer to find optimal weights and reducing the likelihood of vanishing gradients.
