### 1. What is an activation function in a neural network?
Answer:
- An activation function in a neural network is a mathematical function applied to the output of each neuron or node in a layer, transforming the input signal into an output signal.
- Its purpose is to introduce non-linearity into the network, enabling it to learn complex patterns and relationships in data.
- Without activation functions, the neural network would behave like a linear model, regardless of how many layers it has, and would be unable to solve non-linear problems.

### 2. Why are activation functions necessary in neural networks?
Answer:
Activation functions are necessary because they introduce non-linearity into the neural network. Without non-linearity, no matter how many layers are added to the network, it would essentially just be performing linear transformations (combinations of inputs and weights). Non-linearity enables the network to learn and approximate complex functions, making it capable of solving a wide range of tasks, such as classification, regression, and more.

### 3. Can you explain the difference between linear and non-linear activation functions?
Answer:
  - Linear Activation Function: A linear activation function outputs a linear transformation of the input. It doesn't add any non-linearity, so no matter how many layers the network has, the output is still a linear combination of inputs. It is rarely used because it limits the network's ability to model complex data.

                                            f(x)=ax+b

  - Non-linear Activation Functions: These functions allow the network to approximate more complex patterns and relationships in data. They introduce non-linearity into the network, which is crucial for tasks such as image recognition, language processing, etc.<br>
Example: Sigmoid, ReLU, Tanh, etc.

### 4. What are some commonly used activation functions in neural networks?
Answer:
Here are some of the most commonly used activation functions:

  - Sigmoid: Maps input values between 0 and 1. Commonly used in the output layer for binary classification.
  - Tanh: Similar to the sigmoid but maps input values between -1 and 1. It is zero-centered, which helps in faster convergence compared to sigmoid.
  - ReLU (Rectified Linear Unit): The most popular activation function in deep networks. It outputs the input directly if it is positive, otherwise, it outputs zero.
  - Leaky ReLU: A variant of ReLU that allows a small, non-zero gradient when the input is negative, helping to avoid the "dying ReLU" problem.
  - Softmax: Often used in the output layer for multi-class classification. It converts raw output scores into probabilities that sum up to 1.

### 5. What is the ReLU (Rectified Linear Unit) activation function? Why is it so popular?
Answer:
The ReLU activation function is defined as:

                        f(x)=max(0,x)
It outputs the input directly if it is positive, and for any negative input, it outputs zero.<br>
Why is it popular?:
  + Computational efficiency: ReLU is computationally efficient because it involves simple thresholding.
  + Avoiding the vanishing gradient problem: Unlike sigmoid or tanh, ReLU doesn't squash its input into a small range, which helps with gradient propagation during backpropagation.
  + Sparse activation: Since ReLU outputs zero for all negative values, it can lead to sparse activations (a large portion of neurons being inactive), making the network more efficient.

### 6. What are the problems with the ReLU activation function?
Answer:
ReLU is widely used but has some drawbacks:

- Dying ReLU Problem: Neurons with negative inputs will output zero, and their gradients will be zero during backpropagation. This means they wonâ€™t contribute to learning, potentially leading to "dead" neurons that don't update and learn.
- Unbounded Output: Since ReLU has no upper limit, very large values can cause instability in the network, especially when large weights are involved.

Solutions:
- Leaky ReLU: Allows a small slope for negative values to keep gradients flowing.
- Parametric ReLU (PReLU): A variant where the slope for negative values is learned during training.
- ELU (Exponential Linear Unit): It has a smoother curve for negative values, reducing the dying neuron issue.

