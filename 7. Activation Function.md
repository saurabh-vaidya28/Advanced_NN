### 1. What is an activation function in a neural network?
Answer:
- An activation function in a neural network is a mathematical function applied to the output of each neuron or node in a layer, transforming the input signal into an output signal.
- Its purpose is to introduce non-linearity into the network, enabling it to learn complex patterns and relationships in data.
- Without activation functions, the neural network would behave like a linear model, regardless of how many layers it has, and would be unable to solve non-linear problems.

### 2. Why are activation functions necessary in neural networks?
Answer:
Activation functions are necessary because they introduce non-linearity into the neural network. Without non-linearity, no matter how many layers are added to the network, it would essentially just be performing linear transformations (combinations of inputs and weights). Non-linearity enables the network to learn and approximate complex functions, making it capable of solving a wide range of tasks, such as classification, regression, and more.

### 3. Can you explain the difference between linear and non-linear activation functions?
Answer:
- Linear Activation Function: A linear activation function outputs a linear transformation of the input. It doesn't add any non-linearity, so no matter how many layers the network has, the output is still a linear combination of inputs. It is rarely used because it limits the network's ability to model complex data.

                                            f(x)=ax+b

- Non-linear Activation Functions: These functions allow the network to approximate more complex patterns and relationships in data. They introduce non-linearity into the network, which is crucial for tasks such as image recognition, language processing, etc.<br>
Example: Sigmoid, ReLU, Tanh, etc.

### 4. What are some commonly used activation functions in neural networks?
Answer:
Here are some of the most commonly used activation functions:

- Sigmoid: Maps input values between 0 and 1. Commonly used in the output layer for binary classification.
- Tanh: Similar to the sigmoid but maps input values between -1 and 1. It is zero-centered, which helps in faster convergence compared to sigmoid.
- ReLU (Rectified Linear Unit): The most popular activation function in deep networks. It outputs the input directly if it is positive, otherwise, it outputs zero.
- Leaky ReLU: A variant of ReLU that allows a small, non-zero gradient when the input is negative, helping to avoid the "dying ReLU" problem.
- Softmax: Often used in the output layer for multi-class classification. It converts raw output scores into probabilities that sum up to 1.


